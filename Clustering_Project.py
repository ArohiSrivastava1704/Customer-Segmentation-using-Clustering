# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PgXbyVYIQdRvs93OWrteaX5EyPvhvTPk
"""

import pandas as pd
import numpy as np

# Define the number of rows and generate synthetic data
np.random.seed(42)
rows = 100

data = {
    "Country": [f"Country_{i}" for i in range(1, rows + 1)],
    "Birth Rate": np.random.uniform(10, 50, rows),
    "Business Tax": np.random.uniform(0, 30, rows),
    "CO2 Emissions": np.random.uniform(100, 1000, rows),
    "Days to Start Business": np.random.randint(1, 100, rows),
    "Ease of Business": np.random.uniform(1, 10, rows),
    "Energy Usage": np.random.uniform(500, 5000, rows),
    "GDP": np.random.uniform(1000, 100000, rows),
    "Health Exp %GDP": np.random.uniform(2, 15, rows),
    "Health Exp per Capita": np.random.uniform(100, 10000, rows),
    "Hours to Do Tax": np.random.randint(1, 500, rows),
    "Infant Mortality": np.random.uniform(1, 100, rows),
    "Internet Usage": np.random.uniform(0, 100, rows),
    "Lending Rate": np.random.uniform(0, 20, rows),
    "Life Expectancy Female": np.random.uniform(50, 90, rows)
}

# Create DataFrame
df = pd.DataFrame(data)

# Save to CSV
df.to_csv("global_development_data.csv", index=False)
print("Synthetic dataset saved as 'global_development_data.csv'")

!pip install streamlit

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
import streamlit as st

# Load dataset
data = pd.read_csv("global_development_data.csv")

# Inspect dataset
print(data.head())

# Drop unnecessary columns
data_cleaned = data.drop(['Country'], axis=1)

# Handle missing values
data_cleaned.fillna(data_cleaned.mean(), inplace=True)

# Standardize the data
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data_cleaned)

# Correlation heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(data_cleaned.corr(), annot=True, cmap="coolwarm")
plt.title("Correlation Heatmap")
plt.show()

# Distribution plots
for col in data_cleaned.columns:
    sns.histplot(data_cleaned[col], kde=True)
    plt.title(f"Distribution of {col}")
    plt.show()

# Elbow method
inertia = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(data_scaled)
    inertia.append(kmeans.inertia_)

plt.plot(range(1, 11), inertia, marker='o')
plt.title("Elbow Method")
plt.xlabel("Number of clusters")
plt.ylabel("Inertia")
plt.show()

# Optimal K (e.g., 3 from elbow method)
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans_labels = kmeans.fit_predict(data_scaled)

# Hierarchical clustering dendrogram
linked = linkage(data_scaled, method='ward')
dendrogram(linked, truncate_mode='lastp', p=12)
plt.title("Dendrogram")
plt.show()

# Optimal number of clusters (e.g., 3 from dendrogram)
hierarchical_labels = fcluster(linked, 3, criterion='maxclust')

# DBSCAN clustering
dbscan = DBSCAN(eps=1.5, min_samples=5)
dbscan_labels = dbscan.fit_predict(data_scaled)

# Check unique labels assigned by DBSCAN
unique_labels = np.unique(dbscan_labels)
print("Unique labels assigned by DBSCAN:", unique_labels)

# Tuning DBSCAN
from sklearn.neighbors import NearestNeighbors

# Find the optimal eps using k-nearest neighbors (k=4 as a rule of thumb)
nearest_neighbors = NearestNeighbors(n_neighbors=4)
neighbors = nearest_neighbors.fit(data_scaled)
distances, indices = neighbors.kneighbors(data_scaled)

# Sort distances to visualize the knee/elbow point
distances = np.sort(distances[:, 3])
plt.plot(distances)
plt.title("Optimal eps Estimation")
plt.xlabel("Points sorted by distance")
plt.ylabel("4th nearest neighbor distance")
plt.show()

# Updated DBSCAN with tuned parameters
dbscan = DBSCAN(eps=0.5, min_samples=10)  # Adjust eps and min_samples as needed
dbscan_labels = dbscan.fit_predict(data_scaled)

# Verify clusters
print("Unique labels after tuning:", np.unique(dbscan_labels))

if len(np.unique(dbscan_labels)) > 1:
    print("DBSCAN Silhouette Score:", silhouette_score(data_scaled, dbscan_labels))
else:
    print("DBSCAN failed to form multiple clusters.")

# Silhouette scores
print("K-Means Silhouette Score:", silhouette_score(data_scaled, kmeans_labels))
print("Hierarchical Silhouette Score:", silhouette_score(data_scaled, hierarchical_labels))

# DBSCAN clustering
dbscan = DBSCAN(eps=1.5, min_samples=5)
dbscan_labels = dbscan.fit_predict(data_scaled)

# Check the number of unique labels (excluding noise -1)
n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)

# Calculate Silhouette Score if more than one cluster is found
if n_clusters > 1:
    print("DBSCAN Silhouette Score:", silhouette_score(data_scaled, dbscan_labels))
else:
    print("DBSCAN found only one cluster or all noise. Silhouette Score cannot be calculated.")





